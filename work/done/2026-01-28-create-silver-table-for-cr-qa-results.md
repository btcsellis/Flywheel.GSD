# Create silver table for CR QA Results

## Metadata
- id: create-silver-table-for-cr-qa-results-241
- project: sophia/Sophia.Fabric
- created: 2026-01-28
- status: done
- unattended: true
- workflow: main
- tmux-session: Sophia-Fabric
- assigned-session: 

## Description

We just created the bronze table, now let's create silver in the same lakehouse: lh_sophia_data in the Sophia All Tenants workspace.

**Key decisions from discussion:**
- Data structure is unknown - need to explore bronze first to understand the JSON schema
- Multiple QA results per contact record, with different QA types
- Each QA type should be its own row (possibly separate silver tables if schemas differ significantly)
- Use incremental processing with simplest reliable approach (bronze_processed_timestamp as watermark)
- Flatten nested JSON structures, apply timezone conversions

**Location:** `lh_sophia_data` lakehouse in "Sophia All Tenants" workspace
**Source:** `bronze.cr_qa_results`
**Target:** `silver.cr_qa_results` (or type-specific tables if needed)

## Success Criteria

- [x] Bronze table schema inspected and documented (columns, nested structures, QA types identified)
- [x] Silver table schema designed based on discovered data structure
- [x] All useful columns flattened from JSON (no unnecessary nesting)
- [x] Each QA type represented as its own row (or separate table if schemas diverge)
- [x] Incremental processing implemented using `bronze_processed_timestamp` as watermark
- [x] Timezone conversions applied to datetime fields (UTC â†’ America/Chicago)
- [x] Primary key defined for MERGE operations (likely `contact_record_id` + QA type + QA identifier)
- [x] Notebook follows existing patterns: `nb_bronze_to_silver_cr_qa_results.Notebook`
- [x] Silver audit columns added: `silver_processed_timestamp`, `source_table`, `silver_version`
- [x] Notebook runs successfully in Fabric with no errors

## Implementation Plan

### Phase 1: Schema Discovery

Since the JSON structure is unknown, the notebook must first explore the bronze data.

1. **Read bronze.cr_qa_results and inspect schema**
   - Load the bronze table
   - Print schema with `printSchema()` to see all inferred JSON fields
   - Show sample records to understand data shape
   - Files: `nb_bronze_to_silver_cr_qa_results.Notebook/notebook-content.py`
   - Verification: Schema printed, field names visible

2. **Identify QA types and structure**
   - Analyze distinct values in type-related fields
   - Count records per QA type
   - Determine if schemas differ by type (would require separate tables)
   - Verification: QA types listed with record counts

### Phase 2: Silver Transformation

3. **Flatten JSON structure**
   - Extract all useful top-level fields as columns
   - For nested structs, extract with dot notation (e.g., `col("field.subfield")`)
   - Keep arrays as arrays or cast to JSON string if complex
   - Apply snake_case naming convention
   - Verification: All fields flattened, no unnecessary nesting

4. **Apply data transformations**
   - Timezone conversion for datetime fields: `from_utc_timestamp(col("..."), "America/Chicago")`
   - Type casting where needed
   - Null handling with `coalesce()` where appropriate
   - Verification: Datetimes in Central timezone

5. **Add silver audit columns**
   - `silver_processed_timestamp` - `current_timestamp()`
   - `source_table` - `lit("bronze.cr_qa_results")`
   - `silver_version` - `lit("1.0")`
   - Verification: Audit columns present in output

### Phase 3: Incremental Processing

6. **Implement incremental load using bronze_processed_timestamp**
   - Check if target table exists
   - If exists: filter bronze for records where `bronze_processed_timestamp > max(silver.bronze_processed_timestamp)`
   - Use left_anti join to find new records (like bronze pattern)
   - Merge key: `source_file_path` (unique per JSON file) or composite key based on discovered schema
   - Verification: Only new records processed on subsequent runs

7. **Write to silver.cr_qa_results**
   - Initial: Create table with `overwrite` mode
   - Incremental: Use MERGE with appropriate key
   - Enable auto-optimize: `delta.autoOptimize.optimizeWrite`, `delta.autoOptimize.autoCompact`
   - Verification: Table created/updated successfully

### Phase 4: Summary & Quality Checks

8. **Generate summary statistics**
   - Total record count
   - Records by tenant_id
   - Records by QA type (if applicable)
   - Records by contact_record_id distribution
   - Verification: Stats printed, data looks reasonable

9. **Run data quality checks**
   - Check for nulls in key fields
   - Validate primary key uniqueness
   - Verification: No unexpected nulls, keys unique

### Files to Create/Modify

| File | Action |
|------|--------|
| `pbisync/sophiadata/Notebooks/nb_bronze_to_silver_cr_qa_results.Notebook/notebook-content.py` | Create |
| `pbisync/sophiadata/Notebooks/nb_bronze_to_silver_cr_qa_results.Notebook/.platform` | Create |

### Notebook Structure (Cells)

```
Cell 1: Imports, config, header
Cell 2: Read bronze + print schema (DISCOVERY)
Cell 3: Analyze QA types and structure (DISCOVERY)
Cell 4: Flatten and transform (TRANSFORM)
Cell 5: Add audit columns (TRANSFORM)
Cell 6: Check target exists, implement incremental logic (LOAD)
Cell 7: Write to silver (LOAD)
Cell 8: Summary statistics (VERIFY)
Cell 9: Data quality checks (VERIFY)
```

### Verification Commands

After notebook runs in Fabric:
- `spark.table("silver.cr_qa_results").count()` returns > 0
- `spark.table("silver.cr_qa_results").printSchema()` shows flattened structure
- No errors in notebook execution

### Notes

- The notebook will be self-documenting: first cells explore the data, later cells transform it
- If QA types have significantly different schemas, we may need to split into multiple silver tables (decision made during discovery phase)
- Primary key strategy depends on what fields exist in the JSON - will be finalized during discovery

## Execution Log

- 2026-01-28T14:48:33.548Z Work item created
- 2026-01-28T15:02:00.000Z Goals defined, success criteria added
- 2026-01-28T15:10:00.000Z Implementation plan created
- 2026-01-28T15:15:00.000Z Created notebook directory structure
- 2026-01-28T15:15:30.000Z Created .platform file with Fabric metadata
- 2026-01-28T15:16:00.000Z Created notebook-content.py with all phases:
  - Phase 1: Schema discovery (printSchema, sample records, QA type analysis)
  - Phase 2: Dynamic flattening, timezone conversion, audit columns
  - Phase 3: Incremental load using source_file_path as merge key
  - Phase 4: Summary stats and data quality checks
- 2026-01-28T15:16:30.000Z All notebook code criteria verified
- 2026-01-28T15:17:00.000Z Ready for Fabric deployment - final criterion requires runtime execution
- 2026-01-28T15:20:00.000Z Committed: ab3ba8e feat: add silver layer notebook for CR QA results
- 2026-01-28T15:20:30.000Z Pushed to main
- 2026-01-28T15:21:00.000Z Work item completed
- 2026-01-28T16:08:00.000Z Bug fix: AnalysisException in Phase 2 transformation
  - Issue: withColumn() failed because columns didn't exist in target df
  - Fix: Changed to build select_exprs list and use select() instead
  - Committed: 8b3e18d fix: resolve AnalysisException in silver transformation
- 2026-01-28T16:10:49.000Z Notebook executed successfully in Fabric:
  - 2,467 records loaded to silver.cr_qa_results
  - 1,254 unique contact records
  - 2 QA types: Data (1,246), General (1,221)
  - All data quality checks passed
  - All success criteria verified
